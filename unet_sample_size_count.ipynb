{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 09:59:17.239528: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-14 09:59:17.399602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-14 09:59:21.743780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 09:59:21.785754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 09:59:21.785955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import unetgee as ug\n",
    "from update_training_record import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load the CSV file and extract the \"Model Name\" column\n",
    "def load_model_names(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return set(df['Model Name'].astype(str))  # Convert to string for matching purposes\n",
    "\n",
    "# Step 2: List all folders in the directory\n",
    "def list_folders(base_path):\n",
    "    return [name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name))]\n",
    "\n",
    "# Step 3: Check if the folder's model name is in the CSV\n",
    "def check_folders(csv_model_names, folders, return_existing=False):\n",
    "    pattern = re.compile(r\"10bands_LAEA_samples_(\\d+)\")\n",
    "    print(csv_model_names)\n",
    "    print(folders)\n",
    "    matched_folders = {}\n",
    "    for folder in folders:\n",
    "        match = pattern.search(folder)\n",
    "        if return_existing:\n",
    "            if match and match.group(1) in csv_model_names:\n",
    "                matched_folders[match.group(1)] = folder\n",
    "        else :\n",
    "            if match and match.group(1) not in csv_model_names:\n",
    "                matched_folders[match.group(1)] = folder\n",
    "    return matched_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4020050220', '3020008670', '9020000010', '6020021870', '1020034170', '1020035180', '6020029280', '7020024600', '4020024190', '4020015090', '1020011530', '2020041390', '3020009320', '7020038340', '1020000010', '4020034510', '8020000010', '2020033490', '4020050210', '4020000010', '4020050290', '4020050470', '7020065090', '7020046750', '2020057170', '7020014250', '8020008900', '7020000010', '5020015660', '2020065840', '8020032840', '8020044560', '2020071190', '6020000010', '5020037270', '4020006940', '2020000010', '7020021430', '1020021940', '2020024230', '8020010700', '5020000010', '6020017370', '1020027430', '6020008320', '3020003790', '6020014330', '3020005240', '8020020760', '3020024310', '2020018240', '6020006540', '2020003440', '3020000010', '7020047840', '5020049720', '1020018110', '5020082270', '1020040190', '8020022890'}\n",
      "['10bands_LAEA_samples_1020000010', '10bands_LAEA_samples_1020011530', '10bands_LAEA_samples_1020018110', '10bands_LAEA_samples_1020021940', '10bands_LAEA_samples_1020027430', '10bands_LAEA_samples_1020034170', '10bands_LAEA_samples_1020035180', '10bands_LAEA_samples_1020040190', '10bands_LAEA_samples_2020000010', '10bands_LAEA_samples_2020003440', '10bands_LAEA_samples_2020018240', '10bands_LAEA_samples_2020024230', '10bands_LAEA_samples_2020033490', '10bands_LAEA_samples_2020041390', '10bands_LAEA_samples_2020057170', '10bands_LAEA_samples_2020065840', '10bands_LAEA_samples_2020071190', '10bands_LAEA_samples_3020000010', '10bands_LAEA_samples_3020003790', '10bands_LAEA_samples_3020005240', '10bands_LAEA_samples_3020008670', '10bands_LAEA_samples_3020009320', '10bands_LAEA_samples_3020024310', '10bands_LAEA_samples_4020000010', '10bands_LAEA_samples_4020006940', '10bands_LAEA_samples_4020015090', '10bands_LAEA_samples_4020024190', '10bands_LAEA_samples_4020034510', '10bands_LAEA_samples_4020050210', '10bands_LAEA_samples_4020050220', '10bands_LAEA_samples_4020050290', '10bands_LAEA_samples_4020050470', '10bands_LAEA_samples_5020000010', '10bands_LAEA_samples_5020015660', '10bands_LAEA_samples_5020037270', '10bands_LAEA_samples_5020049720', '10bands_LAEA_samples_5020082270', '10bands_LAEA_samples_6020000010', '10bands_LAEA_samples_6020006540', '10bands_LAEA_samples_6020008320', '10bands_LAEA_samples_6020014330', '10bands_LAEA_samples_6020017370', '10bands_LAEA_samples_6020021870', '10bands_LAEA_samples_6020029280', '10bands_LAEA_samples_7020000010', '10bands_LAEA_samples_7020014250', '10bands_LAEA_samples_7020021430', '10bands_LAEA_samples_7020024600', '10bands_LAEA_samples_7020038340', '10bands_LAEA_samples_7020046750', '10bands_LAEA_samples_7020047840', '10bands_LAEA_samples_7020065090', '10bands_LAEA_samples_8020000010', '10bands_LAEA_samples_8020008900', '10bands_LAEA_samples_8020010700', '10bands_LAEA_samples_8020020760', '10bands_LAEA_samples_8020022890', '10bands_LAEA_samples_8020032840', '10bands_LAEA_samples_8020044560', '10bands_LAEA_samples_9020000010']\n",
      "{'1020000010': '10bands_LAEA_samples_1020000010', '1020011530': '10bands_LAEA_samples_1020011530', '1020018110': '10bands_LAEA_samples_1020018110', '1020021940': '10bands_LAEA_samples_1020021940', '1020027430': '10bands_LAEA_samples_1020027430', '1020034170': '10bands_LAEA_samples_1020034170', '1020035180': '10bands_LAEA_samples_1020035180', '1020040190': '10bands_LAEA_samples_1020040190', '2020000010': '10bands_LAEA_samples_2020000010', '2020003440': '10bands_LAEA_samples_2020003440', '2020018240': '10bands_LAEA_samples_2020018240', '2020024230': '10bands_LAEA_samples_2020024230', '2020033490': '10bands_LAEA_samples_2020033490', '2020041390': '10bands_LAEA_samples_2020041390', '2020057170': '10bands_LAEA_samples_2020057170', '2020065840': '10bands_LAEA_samples_2020065840', '2020071190': '10bands_LAEA_samples_2020071190', '3020000010': '10bands_LAEA_samples_3020000010', '3020003790': '10bands_LAEA_samples_3020003790', '3020005240': '10bands_LAEA_samples_3020005240', '3020008670': '10bands_LAEA_samples_3020008670', '3020009320': '10bands_LAEA_samples_3020009320', '3020024310': '10bands_LAEA_samples_3020024310', '4020000010': '10bands_LAEA_samples_4020000010', '4020006940': '10bands_LAEA_samples_4020006940', '4020015090': '10bands_LAEA_samples_4020015090', '4020024190': '10bands_LAEA_samples_4020024190', '4020034510': '10bands_LAEA_samples_4020034510', '4020050210': '10bands_LAEA_samples_4020050210', '4020050220': '10bands_LAEA_samples_4020050220', '4020050290': '10bands_LAEA_samples_4020050290', '4020050470': '10bands_LAEA_samples_4020050470', '5020000010': '10bands_LAEA_samples_5020000010', '5020015660': '10bands_LAEA_samples_5020015660', '5020037270': '10bands_LAEA_samples_5020037270', '5020049720': '10bands_LAEA_samples_5020049720', '5020082270': '10bands_LAEA_samples_5020082270', '6020000010': '10bands_LAEA_samples_6020000010', '6020006540': '10bands_LAEA_samples_6020006540', '6020008320': '10bands_LAEA_samples_6020008320', '6020014330': '10bands_LAEA_samples_6020014330', '6020017370': '10bands_LAEA_samples_6020017370', '6020021870': '10bands_LAEA_samples_6020021870', '6020029280': '10bands_LAEA_samples_6020029280', '7020000010': '10bands_LAEA_samples_7020000010', '7020014250': '10bands_LAEA_samples_7020014250', '7020021430': '10bands_LAEA_samples_7020021430', '7020024600': '10bands_LAEA_samples_7020024600', '7020038340': '10bands_LAEA_samples_7020038340', '7020046750': '10bands_LAEA_samples_7020046750', '7020047840': '10bands_LAEA_samples_7020047840', '7020065090': '10bands_LAEA_samples_7020065090', '8020000010': '10bands_LAEA_samples_8020000010', '8020008900': '10bands_LAEA_samples_8020008900', '8020010700': '10bands_LAEA_samples_8020010700', '8020020760': '10bands_LAEA_samples_8020020760', '8020022890': '10bands_LAEA_samples_8020022890', '8020032840': '10bands_LAEA_samples_8020032840', '8020044560': '10bands_LAEA_samples_8020044560', '9020000010': '10bands_LAEA_samples_9020000010'}\n"
     ]
    }
   ],
   "source": [
    "csv_path = '/WORK/Codes/global_lake_area/training_records.csv'\n",
    "parent_folder_path = '/WORK/SSD_Data/global_lake_area/unet_sample_10bands_laea_per_basin'\n",
    "\n",
    "csv_model_names = load_model_names(csv_path)\n",
    "folders = list_folders(parent_folder_path)\n",
    "\n",
    "matched_folders = check_folders(csv_model_names, folders, return_existing=True)\n",
    "print(matched_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 09:31:19.937136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-19 09:31:19.937514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-19 09:31:19.937761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-19 09:31:20.490249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-19 09:31:20.490721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-19 09:31:20.490737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-19 09:31:20.491010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-19 09:31:20.491073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9330 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 140335, eval_size: 34946\n",
      "training size: 140335, eval_size: 34946\n",
      "Training size: 140335\n",
      "Eval size: 34946\n",
      "training size: 100964, eval_size: 25903\n",
      "training size: 100964, eval_size: 25903\n",
      "Training size: 100964\n",
      "Eval size: 25903\n",
      "training size: 123338, eval_size: 30881\n",
      "training size: 123338, eval_size: 30881\n",
      "Training size: 123338\n",
      "Eval size: 30881\n",
      "training size: 12947, eval_size: 3337\n",
      "training size: 12947, eval_size: 3337\n",
      "Training size: 12947\n",
      "Eval size: 3337\n",
      "training size: 88253, eval_size: 21994\n",
      "training size: 88253, eval_size: 21994\n",
      "Training size: 88253\n",
      "Eval size: 21994\n",
      "training size: 16572, eval_size: 4747\n",
      "training size: 16572, eval_size: 4747\n",
      "Training size: 16572\n",
      "Eval size: 4747\n",
      "training size: 55058, eval_size: 13328\n",
      "training size: 55058, eval_size: 13328\n",
      "Training size: 55058\n",
      "Eval size: 13328\n",
      "training size: 108574, eval_size: 28365\n",
      "training size: 108574, eval_size: 28365\n",
      "Training size: 108574\n",
      "Eval size: 28365\n",
      "training size: 85089, eval_size: 21373\n",
      "training size: 85089, eval_size: 21373\n",
      "Training size: 85089\n",
      "Eval size: 21373\n",
      "training size: 19721, eval_size: 5274\n",
      "training size: 19721, eval_size: 5274\n",
      "Training size: 19721\n",
      "Eval size: 5274\n",
      "training size: 13812, eval_size: 4057\n",
      "training size: 13812, eval_size: 4057\n",
      "Training size: 13812\n",
      "Eval size: 4057\n",
      "training size: 131650, eval_size: 34637\n",
      "training size: 131650, eval_size: 34637\n",
      "Training size: 131650\n",
      "Eval size: 34637\n",
      "training size: 60722, eval_size: 16300\n",
      "training size: 60722, eval_size: 16300\n",
      "Training size: 60722\n",
      "Eval size: 16300\n",
      "training size: 52141, eval_size: 13072\n",
      "training size: 52141, eval_size: 13072\n",
      "Training size: 52141\n",
      "Eval size: 13072\n",
      "training size: 12255, eval_size: 3219\n",
      "training size: 12255, eval_size: 3219\n",
      "Training size: 12255\n",
      "Eval size: 3219\n",
      "training size: 66488, eval_size: 15960\n",
      "training size: 66488, eval_size: 15960\n",
      "Training size: 66488\n",
      "Eval size: 15960\n",
      "training size: 75508, eval_size: 19397\n",
      "training size: 75508, eval_size: 19397\n",
      "Training size: 75508\n",
      "Eval size: 19397\n",
      "training size: 55471, eval_size: 16443\n",
      "training size: 55471, eval_size: 16443\n",
      "Training size: 55471\n",
      "Eval size: 16443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for hybas_id, folder in matched_folders.items():\n",
    "    hybas_id = int(hybas_id)\n",
    "    current_hybas_id = folder.split('_')[-1]\n",
    "    training_size, eval_size = ug.read_samples(\n",
    "        training_pattern='training',\n",
    "        eval_pattern='eval',\n",
    "        local_sample_folder=os.path.join(parent_folder_path, folder)+'/',\n",
    "        input_bands=['B', 'G', 'R', 'NIR', 'SWIR1', 'SWIR2', 'GSW_Occurrence', 'GSW_Recurrence'],\n",
    "        input_band_scaling_factors=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.01, 0.01],\n",
    "        response=['Water_GSW'],\n",
    "        kernel_size=128,\n",
    "        batch_size=128,\n",
    "        only_sizes=True,\n",
    "        compression_type='GZIP'\n",
    "    )\n",
    "    print('Training size:', training_size)\n",
    "    print('Eval size:', eval_size)\n",
    "\n",
    "    update_training_record(hybas_id, trained_status='N', training_set_size=training_size, evaluation_set_size=eval_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 10:06:06.748887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 10:06:06.749421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 10:06:06.749723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 10:06:07.947161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 10:06:07.947747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 10:06:07.947766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-08-14 10:06:07.948036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-14 10:06:07.949121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9330 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 33089, eval_size: 33089\n",
      "training size: 33089, eval_size: 33089\n",
      "Test size: 33089\n",
      "training size: 29666, eval_size: 29666\n",
      "training size: 29666, eval_size: 29666\n",
      "Test size: 29666\n",
      "training size: 24084, eval_size: 24084\n",
      "training size: 24084, eval_size: 24084\n",
      "Test size: 24084\n",
      "training size: 19432, eval_size: 19432\n",
      "training size: 19432, eval_size: 19432\n",
      "Test size: 19432\n",
      "training size: 30316, eval_size: 30316\n",
      "training size: 30316, eval_size: 30316\n",
      "Test size: 30316\n",
      "training size: 28484, eval_size: 28484\n",
      "training size: 28484, eval_size: 28484\n",
      "Test size: 28484\n",
      "training size: 24671, eval_size: 24671\n",
      "training size: 24671, eval_size: 24671\n",
      "Test size: 24671\n",
      "training size: 33506, eval_size: 33506\n",
      "training size: 33506, eval_size: 33506\n",
      "Test size: 33506\n",
      "training size: 15836, eval_size: 15836\n",
      "training size: 15836, eval_size: 15836\n",
      "Test size: 15836\n",
      "training size: 19530, eval_size: 19530\n",
      "training size: 19530, eval_size: 19530\n",
      "Test size: 19530\n",
      "training size: 9771, eval_size: 9771\n",
      "training size: 9771, eval_size: 9771\n",
      "Test size: 9771\n",
      "training size: 8630, eval_size: 8630\n",
      "training size: 8630, eval_size: 8630\n",
      "Test size: 8630\n",
      "training size: 4373, eval_size: 4373\n",
      "training size: 4373, eval_size: 4373\n",
      "Test size: 4373\n",
      "training size: 4757, eval_size: 4757\n",
      "training size: 4757, eval_size: 4757\n",
      "Test size: 4757\n",
      "training size: 2053, eval_size: 2053\n",
      "training size: 2053, eval_size: 2053\n",
      "Test size: 2053\n",
      "training size: 24799, eval_size: 24799\n",
      "training size: 24799, eval_size: 24799\n",
      "Test size: 24799\n",
      "training size: 30804, eval_size: 30804\n",
      "training size: 30804, eval_size: 30804\n",
      "Test size: 30804\n",
      "training size: 8676, eval_size: 8676\n",
      "training size: 8676, eval_size: 8676\n",
      "Test size: 8676\n",
      "training size: 16179, eval_size: 16179\n",
      "training size: 16179, eval_size: 16179\n",
      "Test size: 16179\n",
      "training size: 1299, eval_size: 1299\n",
      "training size: 1299, eval_size: 1299\n",
      "Test size: 1299\n",
      "training size: 8456, eval_size: 8456\n",
      "training size: 8456, eval_size: 8456\n",
      "Test size: 8456\n",
      "training size: 3334, eval_size: 3334\n",
      "training size: 3334, eval_size: 3334\n",
      "Test size: 3334\n",
      "training size: 22428, eval_size: 22428\n",
      "training size: 22428, eval_size: 22428\n",
      "Test size: 22428\n",
      "training size: 14033, eval_size: 14033\n",
      "training size: 14033, eval_size: 14033\n",
      "Test size: 14033\n",
      "training size: 10258, eval_size: 10258\n",
      "training size: 10258, eval_size: 10258\n",
      "Test size: 10258\n",
      "training size: 18318, eval_size: 18318\n",
      "training size: 18318, eval_size: 18318\n",
      "Test size: 18318\n",
      "training size: 25121, eval_size: 25121\n",
      "training size: 25121, eval_size: 25121\n",
      "Test size: 25121\n",
      "training size: 4575, eval_size: 4575\n",
      "training size: 4575, eval_size: 4575\n",
      "Test size: 4575\n",
      "training size: 14358, eval_size: 14358\n",
      "training size: 14358, eval_size: 14358\n",
      "Test size: 14358\n",
      "training size: 29123, eval_size: 29123\n",
      "training size: 29123, eval_size: 29123\n",
      "Test size: 29123\n",
      "training size: 20731, eval_size: 20731\n",
      "training size: 20731, eval_size: 20731\n",
      "Test size: 20731\n",
      "training size: 5165, eval_size: 5165\n",
      "training size: 5165, eval_size: 5165\n",
      "Test size: 5165\n",
      "training size: 9063, eval_size: 9063\n",
      "training size: 9063, eval_size: 9063\n",
      "Test size: 9063\n",
      "training size: 6100, eval_size: 6100\n",
      "training size: 6100, eval_size: 6100\n",
      "Test size: 6100\n",
      "training size: 2025, eval_size: 2025\n",
      "training size: 2025, eval_size: 2025\n",
      "Test size: 2025\n",
      "training size: 39565, eval_size: 39565\n",
      "training size: 39565, eval_size: 39565\n",
      "Test size: 39565\n",
      "training size: 3979, eval_size: 3979\n",
      "training size: 3979, eval_size: 3979\n",
      "Test size: 3979\n",
      "training size: 7047, eval_size: 7047\n",
      "training size: 7047, eval_size: 7047\n",
      "Test size: 7047\n",
      "training size: 16586, eval_size: 16586\n",
      "training size: 16586, eval_size: 16586\n",
      "Test size: 16586\n",
      "training size: 29088, eval_size: 29088\n",
      "training size: 29088, eval_size: 29088\n",
      "Test size: 29088\n",
      "training size: 34926, eval_size: 34926\n",
      "training size: 34926, eval_size: 34926\n",
      "Test size: 34926\n",
      "training size: 16059, eval_size: 16059\n",
      "training size: 16059, eval_size: 16059\n",
      "Test size: 16059\n",
      "training size: 12725, eval_size: 12725\n",
      "training size: 12725, eval_size: 12725\n",
      "Test size: 12725\n",
      "training size: 3023, eval_size: 3023\n",
      "training size: 3023, eval_size: 3023\n",
      "Test size: 3023\n",
      "training size: 33727, eval_size: 33727\n",
      "training size: 33727, eval_size: 33727\n",
      "Test size: 33727\n",
      "training size: 10549, eval_size: 10549\n",
      "training size: 10549, eval_size: 10549\n",
      "Test size: 10549\n",
      "training size: 7321, eval_size: 7321\n",
      "training size: 7321, eval_size: 7321\n",
      "Test size: 7321\n",
      "training size: 5348, eval_size: 5348\n",
      "training size: 5348, eval_size: 5348\n",
      "Test size: 5348\n",
      "training size: 15847, eval_size: 15847\n",
      "training size: 15847, eval_size: 15847\n",
      "Test size: 15847\n",
      "training size: 18909, eval_size: 18909\n",
      "training size: 18909, eval_size: 18909\n",
      "Test size: 18909\n",
      "training size: 19552, eval_size: 19552\n",
      "training size: 19552, eval_size: 19552\n",
      "Test size: 19552\n",
      "training size: 14700, eval_size: 14700\n",
      "training size: 14700, eval_size: 14700\n",
      "Test size: 14700\n",
      "training size: 3198, eval_size: 3198\n",
      "training size: 3198, eval_size: 3198\n",
      "Test size: 3198\n",
      "training size: 7110, eval_size: 7110\n",
      "training size: 7110, eval_size: 7110\n",
      "Test size: 7110\n",
      "training size: 1930, eval_size: 1930\n",
      "training size: 1930, eval_size: 1930\n",
      "Test size: 1930\n",
      "training size: 580, eval_size: 580\n",
      "training size: 580, eval_size: 580\n",
      "Test size: 580\n",
      "training size: 346, eval_size: 346\n",
      "training size: 346, eval_size: 346\n",
      "Test size: 346\n",
      "training size: 1092, eval_size: 1092\n",
      "training size: 1092, eval_size: 1092\n",
      "Test size: 1092\n",
      "training size: 4, eval_size: 4\n",
      "training size: 4, eval_size: 4\n",
      "Test size: 4\n",
      "training size: 476, eval_size: 476\n",
      "training size: 476, eval_size: 476\n",
      "Test size: 476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for hybas_id, folder in matched_folders.items():\n",
    "    hybas_id = int(hybas_id)\n",
    "    current_hybas_id = folder.split('_')[-1]\n",
    "    test_size, _size = ug.read_samples(\n",
    "        training_pattern='test',\n",
    "        eval_pattern='test',\n",
    "        local_sample_folder=os.path.join(parent_folder_path, folder)+'/',\n",
    "        input_bands=['B', 'G', 'R', 'NIR', 'SWIR1', 'SWIR2', 'GSW_Occurrence', 'GSW_Recurrence'],\n",
    "        input_band_scaling_factors=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.01, 0.01],\n",
    "        response=['Water_GSW'],\n",
    "        kernel_size=128,\n",
    "        batch_size=128,\n",
    "        only_sizes=True,\n",
    "        compression_type='GZIP'\n",
    "    )\n",
    "    print('Test size:', test_size)\n",
    "\n",
    "    update_training_record(hybas_id, test_set_size=test_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
